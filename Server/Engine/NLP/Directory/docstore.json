{"docstore/metadata": {"24fbb714-8940-4955-b95d-7661e030525b": {"doc_hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "93ddd632-3293-495c-82b9-a3dca7b8ec76": {"doc_hash": "98d9f604a6820f77ffe76f16096bf0d9cd8651f4be5e60b81b8a71cfb3fb3bd2", "ref_doc_id": "24fbb714-8940-4955-b95d-7661e030525b"}, "4148c5dd-8c31-4169-995f-e19e06df70ce": {"doc_hash": "22ca324bb9e710ac4599964d0c1b45870b8aa7067bce2412ddf68b22b1477098", "ref_doc_id": "24fbb714-8940-4955-b95d-7661e030525b"}, "5bd3cd80-e863-41be-a586-813bf779da6b": {"doc_hash": "1c5c390269e999fcb374b5bb76e320bd9d7dcd0ef9cb7f0285dd730662710804", "ref_doc_id": "24fbb714-8940-4955-b95d-7661e030525b"}, "8c5f4687-9335-4e4f-80e1-a3244209b483": {"doc_hash": "e81cd9aac283ce3e153adf8ee526b6e5b48db8a8a84be8e29d5d0dd2ad1b2736", "ref_doc_id": "24fbb714-8940-4955-b95d-7661e030525b"}, "d8b5e247-80be-4672-81a5-8b733be7143e": {"doc_hash": "df302a1f39b26c661805c9d4a80a4b4a1b35d925abae2f26fe9c8e54897f0119", "ref_doc_id": "24fbb714-8940-4955-b95d-7661e030525b"}}, "docstore/data": {"93ddd632-3293-495c-82b9-a3dca7b8ec76": {"__data__": {"id_": "93ddd632-3293-495c-82b9-a3dca7b8ec76", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24fbb714-8940-4955-b95d-7661e030525b", "node_type": null, "metadata": {}, "hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "3": {"node_id": "4148c5dd-8c31-4169-995f-e19e06df70ce", "node_type": null, "metadata": {}, "hash": "22ca324bb9e710ac4599964d0c1b45870b8aa7067bce2412ddf68b22b1477098"}}, "hash": "98d9f604a6820f77ffe76f16096bf0d9cd8651f4be5e60b81b8a71cfb3fb3bd2", "text": "Hide navigation sidebarHide table of contents sidebar\nLlamaIndex \ud83e\udd99 0.8.41\nGETTING STARTED\n\nInstallation and Setup\nStarter Tutorial\nHigh-Level Concepts\nCustomization Tutorial\nEND-TO-END TUTORIALS\n\nBasic Usage Pattern\nOne-Click Observability\nPrincipled Development Practices\nDiscover LlamaIndex Video Series\nFinetuning\nBuilding RAG from Scratch (Lower-Level)\nUse Cases\nINDEX/DATA MODULES\n\nData Connectors (LlamaHub)\nDocuments / Nodes\nNode Parser\nStorage\nIndexes\nQUERY MODULES\n\nQuery Engine\nChat Engine\nRetriever\nRouters\nNode Postprocessor\nResponse Synthesizer\nStructured Outputs\nAGENT MODULES\n\nData Agents\nTools\nMODEL MODULES\n\nLLM\nEmbeddings\nPrompts\nSUPPORTING MODULES\n\nServiceContext\nCallbacks\nEvaluation\nCost Analysis\nPlayground\nDEVELOPMENT\n\nContributing to LlamaIndex\nDocumentation Guide\nPrivacy and Security\nChangeLog\nCOMMUNITY\n\nIntegrations\nApp Showcase\nAPI REFERENCE\n\nAPI Reference\nIndices\nQuerying an Index\nNode\nLLM Predictors\nLLMs\nPrompt Templates\nEmbeddings\nOpenAIEmbedding\nHuggingFaceEmbedding\nOptimumEmbedding\nInstructorEmbedding\nLangchainEmbedding\nGoogleUnivSentEncoderEmbedding\nNode Postprocessor\nStorage Context\nComposability\nData Connectors\nService Context\nEmbeddings\nOpenAIEmbedding\nHuggingFaceEmbedding\nOptimumEmbedding\nInstructorEmbedding\nLangchainEmbedding\nGoogleUnivSentEncoderEmbedding\nNode Parser\nPromptHelper\nLLMs\nCallbacks\nStructured Index Configuration\nEvaluation\nResponse\nPlayground\nFinetuning\nMemory\nExample Notebooks\nLangchain Integrations\nDeprecated Terms\nSponsored: EthicalAds\nMonetize your audience: Fund an OSS project or website with EthicalAds, a privacy-first ad network\nAd by EthicalAds   \u00b7   \u2139\ufe0f\n v: latest\nBack to top\nEdit this page\nToggle Light / Dark / Auto color theme\nPromptHelper\nGeneral prompt helper that can help deal with LLM context window token limitations.\n\nAt its core, it calculates available context size by starting with the context window size of an LLM and reserve token space for the prompt template, and the output.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4148c5dd-8c31-4169-995f-e19e06df70ce": {"__data__": {"id_": "4148c5dd-8c31-4169-995f-e19e06df70ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24fbb714-8940-4955-b95d-7661e030525b", "node_type": null, "metadata": {}, "hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "2": {"node_id": "93ddd632-3293-495c-82b9-a3dca7b8ec76", "node_type": null, "metadata": {}, "hash": "98d9f604a6820f77ffe76f16096bf0d9cd8651f4be5e60b81b8a71cfb3fb3bd2"}, "3": {"node_id": "5bd3cd80-e863-41be-a586-813bf779da6b", "node_type": null, "metadata": {}, "hash": "1c5c390269e999fcb374b5bb76e320bd9d7dcd0ef9cb7f0285dd730662710804"}}, "hash": "22ca324bb9e710ac4599964d0c1b45870b8aa7067bce2412ddf68b22b1477098", "text": "It provides utility for \u201crepacking\u201d text chunks (retrieved from index) to maximally make use of the available context window (and thereby reducing the number of LLM calls needed), or truncating them so that they fit in a single LLM call.\n\npydantic model llama_index.indices.prompt_helper.PromptHelper\nPrompt helper.\n\nGeneral prompt helper that can help deal with LLM context window token limitations.\n\nAt its core, it calculates available context size by starting with the context window size of an LLM and reserve token space for the prompt template, and the output.\n\nIt provides utility for \u201crepacking\u201d text chunks (retrieved from index) to maximally make use of the available context window (and thereby reducing the number of LLM calls needed), or truncating them so that they fit in a single LLM call.\n\nPARAMETERS\ncontext_window (int) \u2013 Context window for the LLM.\n\nnum_output (int) \u2013 Number of outputs for the LLM.\n\nchunk_overlap_ratio (float) \u2013 Chunk overlap as a ratio of chunk size\n\nchunk_size_limit (Optional[int]) \u2013 Maximum chunk size to use.\n\ntokenizer (Optional[Callable[[str], List]]) \u2013 Tokenizer to use.\n\nseparator (str) \u2013 Separator for text splitter\n\nShow JSON schema\nFIELDS\nchunk_overlap_ratio (float)\n\nchunk_size_limit (Optional[int])\n\ncontext_window (int)\n\nnum_output (int)\n\nseparator (str)\n\nfield chunk_overlap_ratio: float = 0.1\nThe percentage token amount that each chunk should overlap.\n\nfield chunk_size_limit: Optional[int] = None\nThe maximum size of a chunk.\n\nfield context_window: int = 3900\nThe maximum context size that will get sent to the LLM.\n\nfield num_output: int = 256\nThe amount of token-space to leave in input for generation.\n\nfield separator: str = ' '\nThe separator when chunking tokens.\n\nclassmethod class_name() \u2192 str\nGet the class name, used as a unique ID in serialization.\n\nThis provides a key that makes serialization robust against actual class name changes.\n\nclassmethod construct(_fields_set: Optional[SetStr] = None, **values: Any) \u2192 Model\nCreates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data. Default values are respected, but no other validation is performed.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bd3cd80-e863-41be-a586-813bf779da6b": {"__data__": {"id_": "5bd3cd80-e863-41be-a586-813bf779da6b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24fbb714-8940-4955-b95d-7661e030525b", "node_type": null, "metadata": {}, "hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "2": {"node_id": "4148c5dd-8c31-4169-995f-e19e06df70ce", "node_type": null, "metadata": {}, "hash": "22ca324bb9e710ac4599964d0c1b45870b8aa7067bce2412ddf68b22b1477098"}, "3": {"node_id": "8c5f4687-9335-4e4f-80e1-a3244209b483", "node_type": null, "metadata": {}, "hash": "e81cd9aac283ce3e153adf8ee526b6e5b48db8a8a84be8e29d5d0dd2ad1b2736"}}, "hash": "1c5c390269e999fcb374b5bb76e320bd9d7dcd0ef9cb7f0285dd730662710804", "text": "Default values are respected, but no other validation is performed. Behaves as if Config.extra = \u2018allow\u2019 was set since it adds all passed values\n\ncopy(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, update: Optional[DictStrAny] = None, deep: bool = False) \u2192 Model\nDuplicate a model, optionally choose which fields to include, exclude and change.\n\nPARAMETERS\ninclude \u2013 fields to include in new model\n\nexclude \u2013 fields to exclude from new model, as with values this takes precedence over include\n\nupdate \u2013 values to change/add in the new model. Note: the data is not validated before creating the new model: you should trust this data\n\ndeep \u2013 set to True to make a deep copy of the model\n\nRETURNS\nnew model instance\n\ndict(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) \u2192 DictStrAny\nGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n\nclassmethod from_dict(data: Dict[str, Any], **kwargs: Any) \u2192 Self\nclassmethod from_json(data_str: str, **kwargs: Any) \u2192 Self\nclassmethod from_llm_metadata(llm_metadata: LLMMetadata, chunk_overlap_ratio: float = 0.1, chunk_size_limit: Optional[int] = None, tokenizer: Optional[Callable[[str], List]] = None, separator: str = ' ') \u2192 PromptHelper\nCreate from llm predictor.\n\nThis will autofill values like context_window and num_output.\n\nclassmethod from_orm(obj: Any) \u2192 Model\nget_text_splitter_given_prompt(prompt: BasePromptTemplate, num_chunks: int = 1, padding: int = 5) \u2192 TokenTextSplitter\nGet text splitter configured to maximally pack available context window, taking into account of given prompt, and desired number of chunks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c5f4687-9335-4e4f-80e1-a3244209b483": {"__data__": {"id_": "8c5f4687-9335-4e4f-80e1-a3244209b483", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24fbb714-8940-4955-b95d-7661e030525b", "node_type": null, "metadata": {}, "hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "2": {"node_id": "5bd3cd80-e863-41be-a586-813bf779da6b", "node_type": null, "metadata": {}, "hash": "1c5c390269e999fcb374b5bb76e320bd9d7dcd0ef9cb7f0285dd730662710804"}, "3": {"node_id": "d8b5e247-80be-4672-81a5-8b733be7143e", "node_type": null, "metadata": {}, "hash": "df302a1f39b26c661805c9d4a80a4b4a1b35d925abae2f26fe9c8e54897f0119"}}, "hash": "e81cd9aac283ce3e153adf8ee526b6e5b48db8a8a84be8e29d5d0dd2ad1b2736", "text": "json(*, include: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, exclude: Optional[Union[AbstractSetIntStr, MappingIntStrAny]] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) \u2192 unicode\nGenerate a JSON representation of the model, include and exclude arguments as per dict().\n\nencoder is an optional function to supply as default to json.dumps(), other arguments as per json.dumps().\n\nclassmethod parse_file(path: Union[str, Path], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) \u2192 Model\nclassmethod parse_obj(obj: Any) \u2192 Model\nclassmethod parse_raw(b: Union[str, bytes], *, content_type: unicode = None, encoding: unicode = 'utf8', proto: Protocol = None, allow_pickle: bool = False) \u2192 Model\nrepack(prompt: BasePromptTemplate, text_chunks: Sequence[str], padding: int = 5) \u2192 List[str]\nRepack text chunks to fit available context window.\n\nThis will combine text chunks into consolidated chunks that more fully \u201cpack\u201d the prompt template given the context_window.\n\nclassmethod schema(by_alias: bool = True, ref_template: unicode = '#/definitions/{model}') \u2192 DictStrAny\nclassmethod schema_json(*, by_alias: bool = True, ref_template: unicode = '#/definitions/{model}', **dumps_kwargs: Any) \u2192 unicode\nto_dict(**kwargs: Any) \u2192 Dict[str, Any]\nto_json(**kwargs: Any) \u2192 str\ntruncate(prompt: BasePromptTemplate, text_chunks: Sequence[str], padding: int = 5) \u2192 List[str]\nTruncate text chunks to fit available context window.\n\nclassmethod update_forward_refs(**localns: Any) \u2192 None\nTry to update ForwardRefs on fields based on this Model, globalns and localns.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8b5e247-80be-4672-81a5-8b733be7143e": {"__data__": {"id_": "d8b5e247-80be-4672-81a5-8b733be7143e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24fbb714-8940-4955-b95d-7661e030525b", "node_type": null, "metadata": {}, "hash": "c429138ece0b79067b17373473fe206951ef897c2f80de02ced109c503dd6790"}, "2": {"node_id": "8c5f4687-9335-4e4f-80e1-a3244209b483", "node_type": null, "metadata": {}, "hash": "e81cd9aac283ce3e153adf8ee526b6e5b48db8a8a84be8e29d5d0dd2ad1b2736"}}, "hash": "df302a1f39b26c661805c9d4a80a4b4a1b35d925abae2f26fe9c8e54897f0119", "text": "classmethod validate(value: Any) \u2192 Model\nNext\nCallbacks\nPrevious\nNode Parser\nCopyright \u00a9 2022, Jerry Liu\nMade with Sphinx and @pradyunsg's Furo\nON THIS PAGE\nPromptHelper\nPromptHelper.chunk_overlap_ratio\nPromptHelper.chunk_size_limit\nPromptHelper.context_window\nPromptHelper.num_output\nPromptHelper.separator\nPromptHelper.class_name()\nPromptHelper.construct()\nPromptHelper.copy()\nPromptHelper.dict()\nPromptHelper.from_dict()\nPromptHelper.from_json()\nPromptHelper.from_llm_metadata()\nPromptHelper.from_orm()\nPromptHelper.get_text_splitter_given_prompt()\nPromptHelper.json()\nPromptHelper.parse_file()\nPromptHelper.parse_obj()\nPromptHelper.parse_raw()\nPromptHelper.repack()\nPromptHelper.schema()\nPromptHelper.schema_json()\nPromptHelper.to_dict()\nPromptHelper.to_json()\nPromptHelper.truncate()\nPromptHelper.update_forward_refs()\nPromptHelper.validate()\n\ud83e\udd99\n\nCTRL + K", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"24fbb714-8940-4955-b95d-7661e030525b": {"node_ids": ["93ddd632-3293-495c-82b9-a3dca7b8ec76", "4148c5dd-8c31-4169-995f-e19e06df70ce", "5bd3cd80-e863-41be-a586-813bf779da6b", "8c5f4687-9335-4e4f-80e1-a3244209b483", "d8b5e247-80be-4672-81a5-8b733be7143e"], "metadata": {}}}}